{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from typing import Dict, List, Tuple, NamedTuple\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import cvxpy as cp\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from skimage.filters import threshold_otsu\n",
    "import csv\n",
    "import logging\n",
    "import time\n",
    "import psutil\n",
    "from dataclasses import dataclass\n",
    "import json "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Simulation Study Triplet Values ###\n",
    "SIMULATION_STUDY = \"example\"\n",
    "NUMBER_CHANEGPOINTS = 2\n",
    "NUMBER_NODES = 6\n",
    "NUMBER_ORIGINAL_SAMPLES = 500\n",
    "\n",
    "###Â Hyper Parameter Setup ###\n",
    "GAUSSIAN_SIGMA = NUMBER_ORIGINAL_SAMPLES / (100)\n",
    "\n",
    "### Logging Setup ###\n",
    "SEVRER_HOME_DIRECTORY = '/Users/jamiecuin/Documents/University/Imperial/MSc Statistics/Summer Project/EGM Changepoint Detection/Code/Simulation Studies'\n",
    "SIMULATION_DIRECTORY = os.path.join(SEVRER_HOME_DIRECTORY, f'simulation_study_{SIMULATION_STUDY}', f'number_changepoints_{NUMBER_CHANEGPOINTS}', f'number_nodes_{NUMBER_NODES}', f'number_samples_{NUMBER_ORIGINAL_SAMPLES}')\n",
    "CSV_DIRECTORY = os.path.join(SIMULATION_DIRECTORY, 'csv_files')\n",
    "PLOT_DIRECTORY = os.path.join(SIMULATION_DIRECTORY, 'plots')\n",
    "RESULTS_DIRECTORY = os.path.join(SIMULATION_DIRECTORY, 'results')\n",
    "\n",
    "log_file_path = os.path.join(SIMULATION_DIRECTORY, 'simulation_log.txt')\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filename=log_file_path,\n",
    "                    filemode='w')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Simulation Study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimulationParams:\n",
    "    gaussian_sigma: float\n",
    "    total_original_samples: int\n",
    "\n",
    "def log_memory_usage():\n",
    "    process = psutil.Process()\n",
    "    memory_info = process.memory_info()\n",
    "    logger.info(f\"Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "def load_data(csv_directory_path: str, results_directory_path: str) -> Tuple[Dict[Tuple[int, int], Dict[int, pd.DataFrame]], Dict[int, List[int]]]:\n",
    "    logger.info(f\"Loading data from directory: {csv_directory_path}\")\n",
    "    csv_files = glob.glob(os.path.join(csv_directory_path, '*.csv'))\n",
    "    dataframes = defaultdict(dict)\n",
    "    for file in csv_files:\n",
    "        if 'master_changepoint_locations' in file:\n",
    "            continue\n",
    "        df = pd.read_csv(file)\n",
    "        filename = os.path.basename(file)\n",
    "        parts = filename.split('_')\n",
    "        \n",
    "        if len(parts) == 7:\n",
    "            simulation = int(parts[4][10:])\n",
    "            permutation = int(parts[5][11:])\n",
    "            node = int(parts[6].split('.')[0][4:])\n",
    "            dataframes[(simulation, permutation)][node] = df\n",
    "        else:\n",
    "            logger.info(f\"Warning: Unexpected filename format {filename}. Skipping this file.\")\n",
    "    \n",
    "    changepoint_file = os.path.join(results_directory_path, 'master_changepoint_locations.csv')\n",
    "    if not os.path.exists(changepoint_file):\n",
    "        logger.info(f\"Warning: master_changepoint_locations.csv not found in {results_directory_path}\")\n",
    "        changepoints = {}\n",
    "    else:\n",
    "        changepoints_df = pd.read_csv(changepoint_file)\n",
    "        changepoints = {}\n",
    "        for _, row in changepoints_df.iterrows():\n",
    "            sim = row['simulation']\n",
    "            cp_indices = row['changepoint_indices']\n",
    "            if pd.isna(cp_indices) or cp_indices == '':\n",
    "                changepoints[sim] = []\n",
    "            else:\n",
    "                try:\n",
    "                    changepoints[sim] = [int(cp_indices)]\n",
    "                except ValueError:\n",
    "                    changepoints[sim] = [int(x) for x in cp_indices.split(';') if x.strip()]\n",
    "\n",
    "    logger.info(f\"Loaded {len(dataframes)} simulations and {len(changepoints)} changepoints\")\n",
    "    \n",
    "    return dataframes, changepoints\n",
    "\n",
    "def fused_lasso(data, m, lambda1_value, lambda2_value, return_all: bool = False):\n",
    "    n, p = data.shape\n",
    "    y = data[:, m].astype(np.float64)\n",
    "    X = np.delete(data.copy(), m, axis=1).T.astype(np.float64)\n",
    "    \n",
    "    lambda1 = cp.Parameter(nonneg=True)\n",
    "    lambda2 = cp.Parameter(nonneg=True)\n",
    "    beta = cp.Variable((p-1, n))\n",
    "    \n",
    "    lasso_penalty = cp.norm1(beta)\n",
    "    fusion_penalty = cp.sum([cp.norm2(beta[:, i] - beta[:, i-1]) for i in range(1, n)])\n",
    "    loss = cp.sum_squares(y - cp.sum(cp.multiply(X, beta), axis=0))\n",
    "    \n",
    "    objective = cp.Minimize(loss + (2 * lambda1 * fusion_penalty) + (2 * lambda2 * lasso_penalty))\n",
    "    problem = cp.Problem(objective)\n",
    "    \n",
    "    lambda1.value = lambda1_value\n",
    "    lambda2.value = lambda2_value\n",
    "    \n",
    "    problem.solve()\n",
    "    \n",
    "    beta_estimated = beta.value\n",
    "    loss_value = loss.value\n",
    "    penalty = (2 * lambda1.value * fusion_penalty.value) + (2 * lambda2.value * lasso_penalty.value)\n",
    "    \n",
    "    if return_all:\n",
    "        return beta_estimated, loss_value, penalty\n",
    "    return beta_estimated\n",
    "\n",
    "def calculate_differences(beta_estimated):\n",
    "    return np.abs(np.diff(beta_estimated, axis=1))\n",
    "\n",
    "def calculate_BIC(loss, beta_estimated, n_samples):\n",
    "    first_differences = calculate_differences(beta_estimated)\n",
    "    bic = n_samples * np.log(loss / n_samples) + np.sum(np.abs(first_differences) > 1e-6) * np.log(n_samples)\n",
    "    return bic\n",
    "\n",
    "def evaluate_params(lambda1_value, lambda2_value, transformed_data, n_transformed_samples, d):\n",
    "    total_BIC = 0\n",
    "    for m in range(d-1):\n",
    "        beta_estimated, loss, penalty = fused_lasso(transformed_data, m, lambda1_value, lambda2_value, return_all=True)\n",
    "        bic = calculate_BIC(loss, beta_estimated, n_transformed_samples)\n",
    "        total_BIC += bic\n",
    "    return lambda1_value, lambda2_value, total_BIC\n",
    "\n",
    "def get_lambda_range(lambda_theoretical: float, n_points: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    lower_bound_lambda = (1/3) * lambda_theoretical\n",
    "    upper_bound_lambda = 3 * lambda_theoretical\n",
    "\n",
    "    lambda_range = np.linspace(lower_bound_lambda, upper_bound_lambda, num=n_points)\n",
    "\n",
    "    return lambda_range\n",
    "\n",
    "\n",
    "def get_regularization_params(transformed_data, n_transformed_samples):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    d = transformed_data.shape[1] + 1\n",
    "    lambda1_theoretical = 1 * n_transformed_samples ** (1/2)\n",
    "    lambda2_theoretical = 2 * np.sqrt(np.log(d-1) / n_transformed_samples)\n",
    "\n",
    "    lambda1_range = get_lambda_range(lambda1_theoretical, n_points=20)\n",
    "    lambda2_range = get_lambda_range(lambda2_theoretical, n_points=20)\n",
    "\n",
    "    lambda_combinations = [(l1, l2) for l1 in lambda1_range for l2 in lambda2_range]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(evaluate_params)(l1, l2, transformed_data, n_transformed_samples, d)\n",
    "        for l1, l2 in tqdm(lambda_combinations, desc=\"Evaluating lambda pairs\")\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Parameter evaluation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    best_lambda1, best_lambda2, best_BIC_sum = min(results, key=lambda x: x[2])\n",
    "    \n",
    "    logger.info(f\"Optimal lambda1: {best_lambda1}, lambda2: {best_lambda2}\")\n",
    "    return best_lambda1, best_lambda2\n",
    "\n",
    "def plot_with_changepoints(data, changepoints, title, xlabel: str = '', ylabel: str = '', filename: str = None):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(data)\n",
    "    for cp in changepoints:\n",
    "        if isinstance(cp, int):\n",
    "            index, color, linestyle = cp, 'r', '--'\n",
    "        elif isinstance(cp, tuple):\n",
    "            index, color, linestyle = cp\n",
    "        plt.axvline(x=index, color=color, linestyle=linestyle, label=f'Changepoint at index {index}')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    if filename:\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    "        logger.info(f\"Plot saved as '{filename}'\")\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def compute_dot_S(tilde_S):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    dot_S = np.zeros(next(iter(tilde_S.values())).shape[0])\n",
    "    for tilde_S_k in tilde_S.values():\n",
    "        dot_S += tilde_S_k\n",
    "    return dot_S\n",
    "\n",
    "def estimate_changepoints(dot_S: np.ndarray, params: SimulationParams, min_threshold: float = 0) -> Tuple[List[int], np.ndarray, float]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    logger.info(\"Estimating changepoints\")\n",
    "    # logger.info(f\"Estimated Sigma: {calculate_sigma_hat(dot_S)}\")\n",
    "    smoothed_data = gaussian_filter1d(dot_S, params.gaussian_sigma)\n",
    "    threshold = threshold_otsu(smoothed_data)\n",
    "    threshold = max(threshold, min_threshold)\n",
    "    peaks, _ = find_peaks(smoothed_data, height=threshold)\n",
    "    logger.info(f\"Estimated {len(peaks)} changepoints\")\n",
    "    return peaks.tolist(), smoothed_data, threshold, peaks\n",
    "\n",
    "def plot_changepoint_estimation(dot_S: np.ndarray, smoothed_data: np.ndarray, \n",
    "                                peaks: List[int], threshold: float,\n",
    "                                sim: int, perm: int, plot_directory: str,\n",
    "                                true_changepoints: List[int]):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(dot_S, label='Non-Smoothed')\n",
    "    plt.plot(smoothed_data, label='Smoothed', linewidth=2)\n",
    "    plt.scatter(peaks, smoothed_data[peaks], color='red', label='Estimated Peaks')\n",
    "    plt.axhline(y=threshold, color='gray', linestyle='--', label='Threshold')\n",
    "    \n",
    "    # Plot true changepoints\n",
    "    for cp in true_changepoints:\n",
    "        plt.axvline(x=cp, color='red', linestyle=':', linewidth=2, label='True Changepoint')\n",
    "    \n",
    "    # Remove duplicate labels\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    plt.title(f'Changepoint Estimation - Simulation {sim}, Permutation {perm}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    filename = os.path.join(plot_directory, f'changepoint_estimation_sim{sim}_perm{perm}.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    logger.info(f\"Changepoint estimation plot saved as '{filename}'\")\n",
    "\n",
    "def compute_tilde_S(dataframe: pd.DataFrame, params: SimulationParams) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    logger.info(f\"Computing tilde_S for dataframe with shape: {dataframe.shape}\")\n",
    "    transformed_data = dataframe.iloc[:, 1:].values\n",
    "    original_index = dataframe.iloc[:, 0].values - 1\n",
    "    d = transformed_data.shape[1] + 1\n",
    "    n_transformed_samples = transformed_data.shape[0]\n",
    "    \n",
    "    lambda1_m, lambda2_m = get_regularization_params(\n",
    "        transformed_data,\n",
    "        n_transformed_samples,\n",
    "        bic_method='BIC'\n",
    "    )\n",
    "    \n",
    "    beta_hat_m = {}\n",
    "    beta_hat_m_differences = {}\n",
    "    for k in range(d-1):\n",
    "        beta_hat_m[k] = fused_lasso(transformed_data, k, lambda1_m, lambda2_m)\n",
    "        beta_hat_m_differences[k] = calculate_differences(beta_hat_m[k])\n",
    "        beta_hat_m_differences[k] = np.insert(beta_hat_m_differences[k], 0, 0, axis=1)\n",
    "    \n",
    "    beta_differences_original_index = defaultdict(lambda: np.zeros(params.total_original_samples))\n",
    "    for k in range(d-1):\n",
    "        for i, difference in enumerate(beta_hat_m_differences[k].T):\n",
    "            beta_differences_original_index[k][original_index[i]] = np.sum(difference)\n",
    "    \n",
    "    tilde_S = np.zeros(params.total_original_samples)\n",
    "    for differences_array in beta_differences_original_index.values():\n",
    "        tilde_S += differences_array\n",
    "    \n",
    "    return tilde_S, lambda1_m, lambda2_m, beta_hat_m, beta_hat_m_differences, beta_differences_original_index\n",
    "\n",
    "def save_beta_hat_m(beta_hat_m, sim, perm, m, results_directory):\n",
    "    sim_perm_dir = os.path.join(results_directory, f\"sim_{sim}_perm_{perm}\")\n",
    "    os.makedirs(sim_perm_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert numpy arrays to lists\n",
    "    beta_hat_m_json = {str(k): beta_hat.tolist() for k, beta_hat in beta_hat_m.items()}\n",
    "    \n",
    "    filename = os.path.join(sim_perm_dir, f\"beta_hat_m_{m}.json\")\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(beta_hat_m_json, f)\n",
    "    \n",
    "    logger.info(f\"Saved beta_hat_m for simulation {sim}, permutation {perm}, dataset {m} to {filename}\")\n",
    "\n",
    "def save_beta_hat_m_differences(beta_hat_m_differences, sim, perm, m, results_directory):\n",
    "    sim_perm_dir = os.path.join(results_directory, f\"sim_{sim}_perm_{perm}\")\n",
    "    os.makedirs(sim_perm_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert numpy arrays to lists\n",
    "    beta_hat_m_differences_json = {str(k): values.tolist() for k, values in beta_hat_m_differences.items()}\n",
    "    \n",
    "    filename = os.path.join(sim_perm_dir, f\"beta_hat_differences_m_{m}.json\")\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(beta_hat_m_differences_json, f)\n",
    "    \n",
    "    logger.info(f\"Saved beta_hat_m_differences for simulation {sim}, permutation {perm}, dataset {m} to {filename}\")\n",
    "\n",
    "def save_beta_differences_original_index(beta_differences_original_index, sim, perm, m, results_directory):\n",
    "    sim_perm_dir = os.path.join(results_directory, f\"sim_{sim}_perm_{perm}\")\n",
    "    os.makedirs(sim_perm_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert numpy arrays to lists\n",
    "    beta_differences_original_index_json = {str(k): values.tolist() for k, values in beta_differences_original_index.items()}\n",
    "    \n",
    "    filename = os.path.join(sim_perm_dir, f\"beta_differences_original_index_m_{m}.json\")\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(beta_differences_original_index_json, f)\n",
    "    \n",
    "    logger.info(f\"Saved beta_differences_original_index for simulation {sim}, permutation {perm}, dataset {m} to {filename}\")\n",
    "\n",
    "def save_tilde_S(tilde_S_dict, sim, perm, results_directory):\n",
    "    sim_perm_dir = os.path.join(results_directory, f\"sim_{sim}_perm_{perm}\")\n",
    "    os.makedirs(sim_perm_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert numpy arrays to lists and include m value\n",
    "    tilde_S_json = {str(m): {\"m\": m, \"values\": values.tolist()} for m, values in tilde_S_dict.items()}\n",
    "    \n",
    "    filename = os.path.join(sim_perm_dir, f\"tilde_S_values.json\")\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(tilde_S_json, f)\n",
    "    \n",
    "    logger.info(f\"Saved tilde_S values for simulation {sim}, permutation {perm} to {filename}\")\n",
    "\n",
    "def save_dot_S_and_smoothed(dot_S, smoothed_data, peaks, threshold, sim, perm, results_directory):\n",
    "    sim_perm_dir = os.path.join(results_directory, f\"sim_{sim}_perm_{perm}\")\n",
    "    os.makedirs(sim_perm_dir, exist_ok=True)\n",
    "    \n",
    "    data_to_save = {\n",
    "        \"dot_S\": dot_S.tolist(),\n",
    "        \"smoothed_dot_S\": smoothed_data.tolist(),\n",
    "        \"peaks\": peaks.tolist(),  # Ensure peaks is a list\n",
    "        \"threshold\": float(threshold)\n",
    "    }\n",
    "    \n",
    "    filename = os.path.join(sim_perm_dir, f\"dot_S_and_smoothed.json\")\n",
    "    try:\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(data_to_save, f)\n",
    "        logger.info(f\"Saved dot_S, smoothed dot_S, peaks, and threshold for simulation {sim}, permutation {perm} to {filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving data to {filename}: {str(e)}\")\n",
    "\n",
    "\n",
    "\n",
    "def plot_beta_hat_m(beta_hat_m, m):\n",
    "    \"\"\"\n",
    "    Plots the beta_hat_m values for each k.\n",
    "    \n",
    "    Parameters:\n",
    "        beta_hat_m (dict): Dictionary where keys are k and values are the coefficient arrays.\n",
    "    \"\"\"\n",
    "    for k, beta_hat in beta_hat_m.items():\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        for i in range(beta_hat.shape[0]):\n",
    "            plt.plot(beta_hat[i, :], label=f'Feature {i+1}' if beta_hat.shape[0] > 1 else 'Coefficient')\n",
    "        \n",
    "        plt.title(f'Coefficient Paths for m={m} k={k}')\n",
    "        plt.xlabel('Index')\n",
    "        plt.ylabel('Coefficient Value')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "def update_results_csv(filename, sim, perm, lambda_pairs, threshold, predicted_changepoints):\n",
    "    file_exists = os.path.isfile(filename)\n",
    "    \n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['simulation', 'permutation', 'lambda_pairs', 'threshold', 'predicted_changepoints']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if not file_exists:\n",
    "            writer.writeheader()\n",
    "        \n",
    "        writer.writerow({\n",
    "            'simulation': sim,\n",
    "            'permutation': perm,\n",
    "            'lambda_pairs': ';'.join([f\"{l1},{l2}\" for l1, l2 in lambda_pairs]),\n",
    "            'threshold': threshold,\n",
    "            'predicted_changepoints': ';'.join(map(str, predicted_changepoints))\n",
    "        })\n",
    "    logger.info(f\"Results updated in CSV: {filename}\")\n",
    "\n",
    "def run_simulation(\n",
    "        sim: int, \n",
    "        perm: int, \n",
    "        dataframes: Dict[int, pd.DataFrame], \n",
    "        changepoints: Dict[int, List[int]], \n",
    "        params: SimulationParams,\n",
    "        results_directory: str,\n",
    "        plot_directory: str\n",
    "    ) -> None:\n",
    "    logger.info(f\"Running Simulation {sim}, Permutation {perm}\")\n",
    "    \n",
    "    tilde_S_dict = {}\n",
    "    lambda_pairs = []\n",
    "\n",
    "    d = len(dataframes)\n",
    "    for m, df in dataframes.items():\n",
    "        logger.info(f\"Analyzing Dataset {m}\")\n",
    "        tilde_S_m, lambda1_m, lambda2_m, beta_hat_m, beta_hat_m_differences, beta_differences_original_index_m = compute_tilde_S(df, params)\n",
    "        tilde_S_dict[m] = tilde_S_m\n",
    "        lambda_pairs.append((lambda1_m, lambda2_m))\n",
    "\n",
    "        # save_beta_hat_m(beta_hat_m, sim, perm, m, results_directory)\n",
    "        # plot_beta_hat_m(beta_hat_m, m)\n",
    "\n",
    "        # save_beta_hat_m_differences(beta_hat_m_differences, sim, perm, m, results_directory)\n",
    "\n",
    "        # save_beta_differences_original_index(beta_differences_original_index_m, sim, perm, m, results_directory)\n",
    "\n",
    "        # save_tilde_S(tilde_S_dict, sim, perm, results_directory)\n",
    "    \n",
    "    true_changepoints = changepoints.get(sim, [])\n",
    "    \n",
    "    dot_S = compute_dot_S(tilde_S_dict)\n",
    "\n",
    "    min_threshold = 2.5e-4 * d * (d-1) # Use this for \\tau_min\n",
    "    \n",
    "    estimated_changepoints, smoothed_data, threshold, peaks = estimate_changepoints(dot_S, params, min_threshold=min_threshold)\n",
    "    \n",
    "    # print(f\"Peaks: {peaks}\")\n",
    "\n",
    "    # save_dot_S_and_smoothed(dot_S, smoothed_data, peaks, threshold, sim, perm, results_directory)\n",
    "    \n",
    "    # Save only the final smoothed estimation plot\n",
    "    plot_changepoint_estimation(dot_S, smoothed_data, estimated_changepoints, threshold, \n",
    "                                sim, perm, plot_directory, true_changepoints)\n",
    "    \n",
    "    csv_results_filename = os.path.join(results_directory, 'simulation_results.csv')\n",
    "    update_results_csv(csv_results_filename, sim, perm, lambda_pairs, threshold, estimated_changepoints)\n",
    "\n",
    "\n",
    "def main():\n",
    "    csv_results_filename = os.path.join(RESULTS_DIRECTORY, 'simulation_results.csv')\n",
    "\n",
    "    if os.path.exists(csv_results_filename):\n",
    "        logger.warning(f\"The results.csv file: {csv_results_filename} already exists. Renaming the existing file.\")\n",
    "        os.rename(csv_results_filename, f\"{csv_results_filename}.bak\")\n",
    "    \n",
    "    logger.info(f\"Starting simulation in directory: {SIMULATION_DIRECTORY}\")\n",
    "    all_simulations_dataframes, changepoints = load_data(csv_directory_path=CSV_DIRECTORY, results_directory_path=RESULTS_DIRECTORY)\n",
    "    \n",
    "    params = SimulationParams(\n",
    "        gaussian_sigma=GAUSSIAN_SIGMA,\n",
    "        total_original_samples=NUMBER_ORIGINAL_SAMPLES\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Simulation parameters: {params}\")\n",
    "    \n",
    "    for (sim, perm), dataframes in tqdm(all_simulations_dataframes.items(), desc=\"Processing simulations\"):\n",
    "        try:\n",
    "            run_simulation(sim, perm, dataframes, changepoints, params, results_directory=RESULTS_DIRECTORY, plot_directory=PLOT_DIRECTORY)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in simulation {sim}, permutation {perm}: {str(e)}\", exc_info=True)\n",
    "    \n",
    "    logger.info(\"All simulations completed\")\n",
    "    log_memory_usage()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    main()\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
