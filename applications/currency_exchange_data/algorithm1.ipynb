{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.signal import find_peaks\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import cvxpy as cp\n",
    "from skimage.filters import threshold_otsu\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "import json\n",
    "import glob\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    CURRENT_DIRECTORY = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    CURRENT_DIRECTORY = os.getcwd()\n",
    "    \n",
    "SIMULATION_DIRECTORY = os.path.join(CURRENT_DIRECTORY, '')\n",
    "RESULTS_DIRECTORY = os.path.join(SIMULATION_DIRECTORY, 'results')\n",
    "PLOT_DIRECTORY = os.path.join(SIMULATION_DIRECTORY, 'plots')\n",
    "\n",
    "# Ensure directories exist\n",
    "os.makedirs(RESULTS_DIRECTORY, exist_ok=True)\n",
    "os.makedirs(PLOT_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    filename=os.path.join(SIMULATION_DIRECTORY, 'simulation_log.txt'),\n",
    "                    filemode='w')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_ORIGINAL_SAMPLES = 1758\n",
    "GAUSSIAN_SIGMA = NUMBER_ORIGINAL_SAMPLES / 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithm 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class SimulationParams:\n",
    "    gaussian_sigma: float\n",
    "    total_original_samples: int\n",
    "\n",
    "def fused_lasso(data, m, lambda1_value, lambda2_value, return_all: bool = False):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    n, p = data.shape\n",
    "    y = data[:, m].astype(np.float64)\n",
    "    X = np.delete(data.copy(), m, axis=1).T.astype(np.float64)\n",
    "    \n",
    "    lambda1 = cp.Parameter(nonneg=True)\n",
    "    lambda2 = cp.Parameter(nonneg=True)\n",
    "    beta = cp.Variable((p-1, n))\n",
    "    \n",
    "    lasso_penalty = cp.norm1(beta)\n",
    "    fusion_penalty = cp.sum([cp.norm2(beta[:, i] - beta[:, i-1]) for i in range(1, n)])\n",
    "    loss = cp.sum_squares(y - cp.sum(cp.multiply(X, beta), axis=0))\n",
    "    \n",
    "    objective = cp.Minimize(loss + (2 * lambda1 * fusion_penalty) + (2 * lambda2 * lasso_penalty))\n",
    "    problem = cp.Problem(objective)\n",
    "    \n",
    "    lambda1.value = lambda1_value\n",
    "    lambda2.value = lambda2_value\n",
    "    \n",
    "    problem.solve()\n",
    "    \n",
    "    beta_estimated = beta.value\n",
    "    loss_value = loss.value\n",
    "    penalty = (2 * lambda1.value * fusion_penalty.value) + (2 * lambda2.value * lasso_penalty.value)\n",
    "    \n",
    "    if return_all:\n",
    "        return beta_estimated, loss_value, penalty\n",
    "    return beta_estimated\n",
    "\n",
    "def calculate_differences(beta_estimated):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    return np.abs(np.diff(beta_estimated, axis=1))\n",
    "\n",
    "def calculate_BIC(loss, beta_estimated, n_samples):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    first_differences = calculate_differences(beta_estimated)\n",
    "    bic = n_samples * np.log(loss / n_samples) + (np.sum(np.abs(first_differences) > 1e-6) * np.log(n_samples) * np.log(np.log(first_differences.shape[1])))\n",
    "    return bic\n",
    "\n",
    "def evaluate_params(lambda1_value, lambda2_value, transformed_data, n_transformed_samples, d):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    total_BIC = 0\n",
    "    for m in range(d-1):\n",
    "        beta_estimated, loss, penalty = fused_lasso(transformed_data, m, lambda1_value, lambda2_value, return_all=True)\n",
    "        bic = calculate_BIC(loss, beta_estimated, n_transformed_samples)\n",
    "        total_BIC += bic\n",
    "    return lambda1_value, lambda2_value, total_BIC\n",
    "\n",
    "def get_lambda_range(lambda_theoretical: float, n_points: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    lower_bound_lambda = (1/3) * lambda_theoretical\n",
    "    upper_bound_lambda = 3 * lambda_theoretical\n",
    "\n",
    "    lambda_range = np.linspace(lower_bound_lambda, upper_bound_lambda, num=n_points)\n",
    "\n",
    "    return lambda_range\n",
    "\n",
    "def get_regularization_params(transformed_data, n_transformed_samples):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    d = transformed_data.shape[1] + 1\n",
    "    lambda1_theoretical = 1 * n_transformed_samples ** (1/2)\n",
    "    lambda2_theoretical = 2 * np.sqrt(np.log(d-1) / n_transformed_samples)\n",
    "\n",
    "    lambda1_range = get_lambda_range(lambda1_theoretical, n_points=40)\n",
    "    lambda2_range = get_lambda_range(lambda2_theoretical, n_points=40)\n",
    "\n",
    "    lambda_combinations = [(l1, l2) for l1 in lambda1_range for l2 in lambda2_range]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    results = Parallel(n_jobs=-1)(\n",
    "        delayed(evaluate_params)(l1, l2, transformed_data, n_transformed_samples, d)\n",
    "        for l1, l2 in tqdm(lambda_combinations, desc=\"Evaluating lambda pairs\")\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    logger.info(f\"Parameter evaluation completed in {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    best_lambda1, best_lambda2, best_BIC_sum = min(results, key=lambda x: x[2])\n",
    "    \n",
    "    logger.info(f\"Optimal lambda1: {best_lambda1}, lambda2: {best_lambda2}\")\n",
    "    return best_lambda1, best_lambda2\n",
    "\n",
    "def compute_tilde_S(dataframe: pd.DataFrame, params: SimulationParams) -> Tuple[np.ndarray, float, float, Dict, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    logger.info(f\"Computing tilde_S for dataframe with shape: {dataframe.shape}\")\n",
    "    transformed_data = dataframe.iloc[:, 1:].values\n",
    "    original_index = dataframe.iloc[:, 0].values - 1\n",
    "    d = transformed_data.shape[1] + 1\n",
    "    n_transformed_samples = transformed_data.shape[0]\n",
    "    \n",
    "    lambda1_m, lambda2_m = get_regularization_params(transformed_data, n_transformed_samples)\n",
    "    \n",
    "    beta_hat_m = {}\n",
    "    beta_hat_m_differences = {}\n",
    "    for k in range(d-1):\n",
    "        beta_hat_m[k] = fused_lasso(transformed_data, k, lambda1_m, lambda2_m)\n",
    "        beta_hat_m_differences[k] = calculate_differences(beta_hat_m[k])\n",
    "        beta_hat_m_differences[k] = np.insert(beta_hat_m_differences[k], 0, 0, axis=1)\n",
    "    \n",
    "    beta_differences_original_index = defaultdict(lambda: np.zeros(params.total_original_samples))\n",
    "    for k in range(d-1):\n",
    "        for i, difference in enumerate(beta_hat_m_differences[k].T):\n",
    "            beta_differences_original_index[k][original_index[i]] = np.sum(difference)\n",
    "    \n",
    "    tilde_S = np.zeros(params.total_original_samples)\n",
    "    for differences_array in beta_differences_original_index.values():\n",
    "        tilde_S += differences_array\n",
    "    \n",
    "    return tilde_S, lambda1_m, lambda2_m, beta_hat_m, beta_hat_m_differences, beta_differences_original_index\n",
    "\n",
    "def compute_dot_S(tilde_S):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    dot_S = np.zeros(next(iter(tilde_S.values())).shape[0])\n",
    "    for tilde_S_k in tilde_S.values():\n",
    "        dot_S += tilde_S_k\n",
    "    return dot_S\n",
    "\n",
    "def estimate_changepoints(\n",
    "        dot_S: np.ndarray, \n",
    "        params: SimulationParams, \n",
    "        min_threshold: float = 0\n",
    "    ) -> Tuple[List[int], np.ndarray, float, np.ndarray]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    logger.info(\"Estimating changepoints\")\n",
    "    smoothed_data = gaussian_filter1d(dot_S, params.gaussian_sigma)\n",
    "    threshold = threshold_otsu(smoothed_data)\n",
    "    threshold = max(threshold, min_threshold)\n",
    "    peaks, _ = find_peaks(smoothed_data, height=threshold)\n",
    "    logger.info(f\"Estimated {len(peaks)} changepoints\")\n",
    "    return peaks.tolist(), smoothed_data, threshold, peaks\n",
    "\n",
    "def plot_changepoint_estimation(\n",
    "        dot_S: np.ndarray, \n",
    "        smoothed_data: np.ndarray, \n",
    "        peaks: List[int], threshold: float,\n",
    "        sim: int, \n",
    "        plot_directory: str,\n",
    "        true_changepoints: List[int]\n",
    "    ):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.plot(dot_S, label='Non-Smoothed')\n",
    "    plt.plot(smoothed_data, label='Smoothed', linewidth=2)\n",
    "    plt.scatter(peaks, smoothed_data[peaks], color='red', label='Estimated Peaks')\n",
    "    plt.axhline(y=threshold, color='gray', linestyle='--', label='Threshold')\n",
    "    \n",
    "    for cp in true_changepoints:\n",
    "        plt.axvline(x=cp, color='red', linestyle=':', linewidth=2, label='True Changepoint')\n",
    "    \n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "    \n",
    "    plt.title(f'Changepoint Estimation - Simulation {sim}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Value')\n",
    "    \n",
    "    filename = os.path.join(plot_directory, f'changepoint_estimation_sim{sim}.png')\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    logger.info(f\"Changepoint estimation plot saved as '{filename}'\")\n",
    "\n",
    "def run_single_simulation(\n",
    "        sim: int, \n",
    "        dataframes: Dict[int, pd.DataFrame], \n",
    "        true_changepoints: List[int], \n",
    "        params: SimulationParams,\n",
    "        plot_directory: str\n",
    "    ) -> Dict:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    logger.info(f\"Running Simulation {sim}\")\n",
    "    \n",
    "    tilde_S_dict = {}\n",
    "    lambda_pairs = []\n",
    "\n",
    "    d = len(dataframes)\n",
    "    for m, df in dataframes.items():\n",
    "        logger.info(f\"Analyzing Dataset {m}\")\n",
    "        tilde_S_m, lambda1_m, lambda2_m, _, _, _ = compute_tilde_S(df, params)\n",
    "        tilde_S_dict[m] = tilde_S_m\n",
    "        lambda_pairs.append((lambda1_m, lambda2_m))\n",
    "    \n",
    "    dot_S = compute_dot_S(tilde_S_dict)\n",
    "    \n",
    "    min_threshold = 2.5e-4 * d * (d-1) # \\tau_min\n",
    "    \n",
    "    estimated_changepoints, smoothed_data, threshold, peaks = estimate_changepoints(dot_S, params, min_threshold=min_threshold)\n",
    "    \n",
    "    plot_changepoint_estimation(dot_S, smoothed_data, estimated_changepoints, threshold, \n",
    "                                sim, plot_directory, true_changepoints)\n",
    "    \n",
    "    results = {\n",
    "        'simulation': sim,\n",
    "        'lambda_pairs': lambda_pairs,\n",
    "        'threshold': threshold,\n",
    "        'estimated_changepoints': estimated_changepoints,\n",
    "        'true_changepoints': true_changepoints\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main(dataframes: Dict[int, pd.DataFrame], true_changepoints: List[int], simulation_id: int):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    params = SimulationParams(\n",
    "        gaussian_sigma=GAUSSIAN_SIGMA,\n",
    "        total_original_samples=NUMBER_ORIGINAL_SAMPLES\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"Starting simulation: {simulation_id}\")\n",
    "    logger.info(f\"Simulation parameters: {params}\")\n",
    "    \n",
    "    results = run_single_simulation(\n",
    "        simulation_id, dataframes, true_changepoints, params, plot_directory=PLOT_DIRECTORY\n",
    "    )\n",
    "    \n",
    "    with open(os.path.join(RESULTS_DIRECTORY, f'simulation_results_{simulation_id}.json'), 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    \n",
    "    logger.info(f\"Simulation {simulation_id} completed. Results saved.\")\n",
    "\n",
    "def load_data(directory_path: str) -> Dict[int, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    csv_files = glob.glob(os.path.join(directory_path, '*.csv'))\n",
    "    dataframes = {}\n",
    "    for file in csv_files:\n",
    "        df = pd.read_csv(file)\n",
    "        node_number = int(os.path.basename(file).split('.')[0][4:])\n",
    "        dataframes[node_number] = df\n",
    "        logger.debug(f'DataFrame for Node {node_number} loaded successfully')\n",
    "    return dataframes\n",
    "\n",
    "def display_data(dataframes: Dict[int, pd.DataFrame], max_plots: int = None):\n",
    "    \"\"\"\n",
    "    TODO\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for k, df in dataframes.items():\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        for i in range(len(df.columns) - 1):\n",
    "            plt.plot(df.iloc[:, 1:].values[:, i], label=f'Variable {i+1}')\n",
    "        plt.title(f'Dataset {k}')\n",
    "        plt.legend()\n",
    "        plt.xlabel('$\\\\mathcal{T}$')\n",
    "        plt.show()\n",
    "        logger.info(f'Plot created for DataFrame {k}')\n",
    "\n",
    "        count += 1\n",
    "\n",
    "        if max_plots is not None and count >= max_plots:\n",
    "            break\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Define path to directory containing transformed datasets\n",
    "    directory_path = 'data/currency_data_subset_approximate_sample_transformed_subset/'\n",
    "    # Load datasets into a dictionary of dataframes\n",
    "    dataframes = load_data(directory_path)\n",
    "    # Visualise datasets\n",
    "    display_data(dataframes, max_plots=1)\n",
    "\n",
    "    simulation_id = 1\n",
    "    # No true change points are known, so pass empty list\n",
    "    true_changepoints = []\n",
    "    main(dataframes, true_changepoints, simulation_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
